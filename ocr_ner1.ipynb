{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0fa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import cv2\n",
    "import easyocr\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d004176f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.utils.serialization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mImage with bounding boxes saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[43mrun_ocr_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTasks\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOCR\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtest2.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use raw string for Windows paths\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 40\u001b[0m, in \u001b[0;36mrun_ocr_pipeline\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     38\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m preprocess_img(image_path)\n\u001b[0;32m     39\u001b[0m original_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m---> 40\u001b[0m raw_text, boxed_image \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_with_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m final_text \u001b[38;5;241m=\u001b[39m clean_text(raw_text)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracted Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mextract_text_with_boxes\u001b[1;34m(image, original_image)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_text_with_boxes\u001b[39m(image: np\u001b[38;5;241m.\u001b[39mndarray, original_image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m---> 11\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43measyocr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     results \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadtext(image)\n\u001b[0;32m     13\u001b[0m     extracted_text: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Tasks\\OCR\\env\\lib\\site-packages\\easyocr\\easyocr.py:214\u001b[0m, in \u001b[0;36mReader.__init__\u001b[1;34m(self, lang_list, gpu, model_storage_directory, user_network_directory, detect_network, recog_network, download_enabled, detector, recognizer, verbose, quantize, cudnn_benchmark)\u001b[0m\n\u001b[0;32m    211\u001b[0m     dict_list[lang] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdict\u001b[39m\u001b[38;5;124m'\u001b[39m, lang \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detector:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recognizer:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recog_network \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32md:\\Tasks\\OCR\\env\\lib\\site-packages\\easyocr\\easyocr.py:271\u001b[0m, in \u001b[0;36mReader.initDetector\u001b[1;34m(self, detector_path)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minitDetector\u001b[39m(\u001b[38;5;28mself\u001b[39m, detector_path):\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcudnn_benchmark\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn_benchmark\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Tasks\\OCR\\env\\lib\\site-packages\\easyocr\\detection.py:78\u001b[0m, in \u001b[0;36mget_detector\u001b[1;34m(trained_model, device, quantize, cudnn_benchmark)\u001b[0m\n\u001b[0;32m     75\u001b[0m net \u001b[38;5;241m=\u001b[39m CRAFT()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     net\u001b[38;5;241m.\u001b[39mload_state_dict(copyStateDict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m))\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quantize:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Tasks\\OCR\\env\\lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.utils.serialization'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading and pre-processing the image\n",
    "def preprocess_img(image_path: str) -> np.ndarray:\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 11, 2)\n",
    "    denoised = cv2.fastNlMeansDenoising(thresh, h=30)\n",
    "    return denoised\n",
    "\n",
    "def extract_text_with_boxes(image: np.ndarray, original_image: np.ndarray) -> Tuple[List[str], np.ndarray]:\n",
    "    reader = easyocr.Reader(['en'], gpu=True)\n",
    "    results = reader.readtext(image)\n",
    "    extracted_text: List[str] = []\n",
    "\n",
    "    for (bbox, text, prob) in results:\n",
    "        if prob > 0.5:  # Confidence threshold\n",
    "            extracted_text.append(text)\n",
    "\n",
    "            (top_left, top_right, bottom_right, bottom_left) = bbox\n",
    "            top_left = tuple(map(int, top_left))\n",
    "            bottom_right = tuple(map(int, bottom_right))\n",
    "            cv2.rectangle(original_image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "            cv2.putText(original_image, text, (top_left[0], top_left[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "\n",
    "    return extracted_text, original_image\n",
    "\n",
    "# Postprocessing\n",
    "def clean_text(text_list: List[str]) -> List[str]:\n",
    "    cleaned: List[str] = []\n",
    "    for text in text_list:\n",
    "        text = re.sub(r'[^\\w\\s\\.\\-\\/]', '', text)\n",
    "        cleaned.append(text.strip())\n",
    "    return cleaned\n",
    "\n",
    "# Main function\n",
    "def run_ocr_pipeline(image_path: str) -> None:\n",
    "    preprocessed = preprocess_img(image_path)\n",
    "    original_image = cv2.imread(image_path)\n",
    "    raw_text, boxed_image = extract_text_with_boxes(preprocessed, original_image)\n",
    "    final_text = clean_text(raw_text)\n",
    "\n",
    "    print(\"\\nExtracted Text:\")\n",
    "    for line in final_text:\n",
    "        print(line)\n",
    "\n",
    "    \n",
    "    output_path = 'output_with_boxes.png'\n",
    "    cv2.imwrite(output_path, boxed_image)\n",
    "    print(f\"\\nImage with bounding boxes saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "run_ocr_pipeline(r'D:\\Tasks\\OCR\\img\\test2.png')  # Use raw string for Windows paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4ffb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55500dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
